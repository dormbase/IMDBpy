
NOTE: the imdbpy2sql.py script, used to populate a database using
the data in the IMDb's plain text data files, is a critical piece
of IMDbPY: it's based on the SQLObject ORM to be database-independent
and contains a lot of tricks to be as fast as possible; however there
are huge margins for improvements; if you want to help, please read the
TODO.txt file and subscribe the imdbpy-devel mailing list at:
  http://imdbpy.sf.net/?page=help#ml
 

  SQL
  ===

Since version 2.1 it's possible to transfer the whole IMDb's
database from the plain text data files into a SQL database.
Starting with version 2.5 every database supported by the SQLObject
Object Relational Manager can be used to store and retrieve
movies and persons information.
This means that MySQL, PostgreSQL, SQLite, Firebird, MAX DB,
Sybase and MSSQL are supported and, as your read this text,
maybe other database backends were added.

You need the SQLObject package, at least version 0.8; even better
if you can download the latest SVN snapshot.

SQLObject home page: http://sqlobject.org/
SVN command to download the latest development version:
  svn co http://svn.colorstudy.com/SQLObject/trunk SQLObject


  SQL DATABASE INSTALLATION
  =========================

Select a mirror of the "The Plain Text Data Files" from
the http://www.imdb.com/interfaces.html page and download
every file in the main directory (beware that the "diffs"
subdirectory contains _a lot_ of files you _don't_ need,
so don't start mirroring everything!).

Starting from release 2.4, you can just download the files you need,
instead of every single file; the files not downloaded will be skipped.
This feature is still quite untested, so please report any bug.

Create a database named "imdb" (or whatever you like),
using the tool provided by your database; as an example, for MySQL
you will use the 'mysqladmin' command:
  # mysqladmin -p create imdb
For PostgreSQL, you have to use the "createdb" command:
  # createdb -W imdb

To create the tables and to populate the database, you must run
the imdbpy2sql.py script:
  # imdbpy2sql.py -d /dir/with/plainTextDataFiles/ -u 'URI'

Where the 'URI' argument is a string representing the connection
to your database, with the schema:
  scheme://[user[:password]@]host[:port]/database[?parameters]

Where 'scheme' is one in "sqlite", "mysql", "postgres", "firebird",
"interbase", "maxdb", "sapdb", "mssql", "sybase".

Some examples:
    mysql://user:password@host/database
    postgres://user:password@host/database
    mysql://host/database?debug=1
    postgres:///full/path/to/socket/database
    postgres://host:5432/database
    sqlite:///full/path/to/database
    sqlite:/C|/full/path/to/database
    sqlite:/:memory:

For other information you can read the SQLObject documentation.


  TIMING
  ======

The performances are hugely dependant upon the underlying Python
module/package used to access the database.
The fastest database appears to be MySQL, with about 95 minutes to
complete on my test system (read below).
A lot of memory (RAM or swap space) is required, in the range of
at least 150/200 megabytes (plus more for the database server).
In the end, the database will require between 1.5GB and 3GB of disc space.

As said, the performances varies greatly using a database server or another:
MySQL, for instance, has an executemany() method of the cursor object
that accept multiple data insertion with a single SQL statement; other
database requires a call to the execute() method for every single row
of data, and they will be much slower - from 2 to 7 times slower than
MySQL.

I've done some tests, using an AMD Athlon 1800+, 512MB of RAM, over a
complete plain text data files set (as of 12 Nov 2006, with about
890.000 titles and over 2.000.000 names):

      database         |  time in minutes: total (insert data/create indexes)
 ----------------------+-----------------------------------------------------
   MySQL 5.0 MyISAM    |  115 (95/20)
   MYSQL 5.0 InnoDB    |  ??? (80/???)
                       |  maybe I've not cofigurated it properly: it
                       |  looks like the creation of the indexes will
                       |  take more than 2 or 3 hours.
   PostgreSQL 8.1      |  190 (177/13)
   SQLite 3.2          | not tested: it seems way too slow: maybe 35 _hours_
                       | to complete; maybe I've misconfigured or I'm
                       | misusing it.

If you have different experiences, please tell me!


  NOTES
  =====

[save the output]
The imdbpy2sql.py will print a lot of debug information on standard output;
you can save it in a file, appending (without quotes) "2>&1 | tee output.txt"

[adult titles]
Beware that, while running, the imdbpy2sql.py script will output a lot
of strings containing both person names and movie titles.  The script
has absolutely no way to know that the processed title is an adult-only
movie, so... if you leave it running and your little daughter runs to you
screaming 'daddy!  daddy!  what kind of animals trains Rocco in the
documentary "Rocco: Animal Trainer 17"???'... well it's not my fault! ;-)


  SQLITE NOTE
  ===========

It seems that, with older versions of the python-sqlite package, the first
run may fail; if you get a DatabaseError exception saying "no such table",
try running again the command with the same arguments.


  SQL USAGE
  =========

Now you can use IMDbPY with the database:
  from imdb import IMDb
  i = IMDb('sql', uri='YOUR_URI_STRING')
  resList = i.search_movie('the incredibles')
  for x in resList: print x
  ti = resList[0]
  i.update(ti)
  print ti['director'][0]

and so on...


